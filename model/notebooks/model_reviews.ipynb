{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1xtYpR5L4Nsc_GBNBN_k5Vq-0PhojtEw7",
     "timestamp": 1684426980748
    },
    {
     "file_id": "1xiMJDWatyC_Lb2YkIEX0uHq2j1QHGx-q",
     "timestamp": 1684307916698
    }
   ],
   "collapsed_sections": [
    "ufzPdoTtNikq",
    "9aHyGuTFgyPO",
    "t6V6ua4QAmAt",
    "scsck0ftF5Vl",
    "hkGgjdv8Qw_p",
    "H63Y-TjyRC7S",
    "440Nd31VTHER",
    "xX1wyEfFkjY3",
    "jeFZlc-HnBKo",
    "g9xikRdtRN1N",
    "mktqckaD3TX1",
    "kU9MfvTQC9wh",
    "7WL5pDmvFyaU"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Анализ и обработка отзывов"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pymorphy3\n",
    "\n",
    "# Инициализация морфологического анализатора для работы с русским языком\n",
    "morph_analyzer = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "\n",
    "class CreateDataset:\n",
    "\tdef __init__(self):\n",
    "\t\t# Определяем ключевые слова для категорий отзывов\n",
    "\t\tself.practice = {'задачи', 'кейс', 'проект', 'задания'}  # Слова, связанные с практикой\n",
    "\t\tself.theory = {'знания', 'лекция', 'материал'}  # Слова, связанные с теорией\n",
    "\t\tself.teacher = {'лектор', 'профессор', 'наставник', 'педагог', 'препод', 'автор',\n",
    "\t\t\t\t\t\t'создатель'}  # Слова, связанные с преподавателем\n",
    "\t\tself.technology = {'ресурсы', 'инструменты', 'доступность',\n",
    "\t\t\t\t\t\t   'интерактивность'}  # Слова, связанные с технологиями\n",
    "\t\tself.relevance = {'современный', 'релевантный', 'новый', 'старый',\n",
    "\t\t\t\t\t\t  'устарел'}  # Слова, связанные с актуальностью\n",
    "\n",
    "\t\t# Список ключевых тем\n",
    "\t\tself.topic_keywords = ['практика', 'теория', 'преподаватель', 'технологии', 'актуальность']\n",
    "\n",
    "\t\t# Создаем словарь, где ключевые темы связываются с набором соответствующих слов\n",
    "\t\tself.topics = {keyword: {keyword} for keyword in self.topic_keywords}\n",
    "\n",
    "\t\t# Обновляем словарь ключевых тем с учетом различных словоформ\n",
    "\t\tself.topics['практика'].update(self.practice)\n",
    "\t\tself.topics['теория'].update(self.theory)\n",
    "\t\tself.topics['преподаватель'].update(self.teacher)\n",
    "\t\tself.topics['технологии'].update(self.technology)\n",
    "\t\tself.topics['актуальность'].update(self.relevance)\n",
    "\n",
    "\tdef names_in_text(self, text: str) -> bool:\n",
    "\t\t\"\"\"\n",
    "        Проверяет, содержатся ли в тексте имена преподавателей.\n",
    "        :param text: Исходный текст\n",
    "        :return: True, если найдено хотя бы одно имя, иначе False\n",
    "        \"\"\"\n",
    "\t\tskip_names = frozenset('паскаль')  # Исключаемое имя (например, Паскаль)\n",
    "\t\tfor word in text:\n",
    "\t\t\tif word not in skip_names:\n",
    "\t\t\t\tparse = morph_analyzer.parse(word)[0]\n",
    "\t\t\t\ttag = parse.tag\n",
    "\t\t\t\t# Проверяем, является ли слово именем или фамилией с высокой вероятностью\n",
    "\t\t\t\tif ('Name' in tag or 'Surn' in tag) and parse.score >= 0.8:\n",
    "\t\t\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "\tdef automatic_annotation(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "\t\t\"\"\"\n",
    "        Автоматическая разметка обучающего набора данных на основе ключевых слов и имен преподавателей.\n",
    "        :param df: DataFrame с текстами для разметки\n",
    "        :return: Размеченный DataFrame\n",
    "        \"\"\"\n",
    "\t\t# Подготовка структуры для хранения данных\n",
    "\t\tdata_all = {t: [] for t in self.topic_keywords}\n",
    "\t\tdata_index = set()\n",
    "\n",
    "\t\t# Проходим по каждому тексту в DataFrame\n",
    "\t\tfor idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "\t\t\tpreprocessed_text = row['PreprocessedText'].split()  # Предобработанный текст\n",
    "\t\t\tdata = []\n",
    "\t\t\tfor key, keywords in self.topics.items():\n",
    "\t\t\t\t# Проверяем наличие ключевых слов в тексте с использованием нечеткого поиска\n",
    "\t\t\t\tif any(True for keyword in keywords for word in preprocessed_text if fuzz.ratio(word, keyword) > 78):\n",
    "\t\t\t\t\tdata_index.add(idx)\n",
    "\t\t\t\t\tdata.append(1)\n",
    "\t\t\t\telif key == 'преподаватель' and self.names_in_text(preprocessed_text):\n",
    "\t\t\t\t\t# Дополнительная проверка на наличие имен преподавателей\n",
    "\t\t\t\t\tdata_index.add(idx)\n",
    "\t\t\t\t\tdata.append(1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Если текст не относится к категории\n",
    "\t\t\t\t\tdata.append(0)\n",
    "\t\t\tif sum(data) > 0:\n",
    "\t\t\t\t# Добавляем данные в соответствующие столбцы\n",
    "\t\t\t\tfor i, key in enumerate(self.topic_keywords):\n",
    "\t\t\t\t\tdata_all[key].append(data[i])\n",
    "\t\treturn pd.DataFrame(data_all, index=sorted(data_index))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обработка текста"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Импортируем датасет с отзывами\n",
    "import pandas as pd\n",
    "from thefuzz import fuzz  # Новая версия библиотеки fuzzywuzzy\n",
    "import nltk\n",
    "\n",
    "# Загрузка необходОгромная благодарность авторам за труд! Полезный материал пра…имых данных для NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Загрузка стоп-слов\n",
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('../data/train_reviews.csv', index_col=0)\n",
    "df['Reviews'] = df['Reviews'].astype(str)\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "\tdef preprocess(self, text: str) -> str:\n",
    "\t\t\"\"\"\n",
    "        Предобрабатывает текст, токенизируя, лемматизируя и удаляя стоп-слова.\n",
    "\n",
    "        :param text: Входной текст, который необходимо предобработать.\n",
    "        :return: Список предобработанных слов.\n",
    "        \"\"\"\n",
    "\t\ttokens = word_tokenize(text.lower())\n",
    "\t\tfiltered_tokens = [\n",
    "\t\t\tnormalized_word\n",
    "\t\t\tfor word in tokens\n",
    "\t\t\tif (normalized_word := morph_analyzer.parse(self._remove_symbols(word))[0].normal_form) not in stop_words\n",
    "\t\t]\n",
    "\n",
    "\t\tfiltered_tokens = \" \".join(filtered_tokens).replace('ё', 'е').split()  # Удаление лишних пробелов\n",
    "\t\treturn \" \".join(filtered_tokens)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _remove_symbols(text: str) -> str:\n",
    "\t\t\"\"\"Удаляет все символы и цифры из строки.\"\"\"\n",
    "\t\tclean_text = \"\".join(char if char.isalnum() and not char.isdigit() else \" \" for char in text)\n",
    "\t\treturn clean_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Применяем автоматическую разметку к `train_reviews.csv`"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text_processor = TextProcessor()\n",
    "\n",
    "# Применение предобработки к отзывам\n",
    "df['PreprocessedText'] = df['Reviews'].apply(text_processor.preprocess)\n",
    "# Создание списка всех стеммированных слов\n",
    "all_words = [word for sublist in df['PreprocessedText'] for word in sublist]\n",
    "unique_words = pd.Series(all_words).value_counts()\n",
    "\n",
    "# Создание DataFrame с индексами и правильными ответами\n",
    "create_dataset = CreateDataset()\n",
    "df_dataset_index = create_dataset.automatic_annotation(df)\n",
    "\n",
    "topic_keywords = create_dataset.topic_keywords\n",
    "\n",
    "# По индексам определяем размеченные отзывы\n",
    "df_marked: pd.DataFrame = df.loc[df_dataset_index.index]\n",
    "for topic in topic_keywords:\n",
    "\tdf_marked[topic] = df_dataset_index[topic]\n",
    "\n",
    "df_marked.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Просмотр количества размеченных текстов"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Всего размеченных текстов -', len(df_dataset_index))\n",
    "for t in topic_keywords:\n",
    "\tprint(t, '-', len(df_dataset_index[df_dataset_index[t] == 1]))\n",
    "\n",
    "# Создание списков для тем и их количества\n",
    "topics = topic_keywords\n",
    "counts = [len(df_dataset_index[df_dataset_index[t] == 1]) for t in topics]\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(topics, counts, color='skyblue')\n",
    "plt.xlabel('Темы')\n",
    "plt.ylabel('Количество размеченных текстов')\n",
    "plt.title('Количество размеченных текстов по темам')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ищем отзывы с нулевыми классами и добавляем их к `df_marked`\n",
    "Нулевые классы - классы не относящиеся ни к одной категории.\n",
    "Нулевые классы делают обучение bert более качественным"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "initial_void_index = list(set(df.index) - set(df_dataset_index.index))\n",
    "initial_void_df = df.loc[initial_void_index]\n",
    "initial_void_df = initial_void_df[\n",
    "\tinitial_void_df['PreprocessedText'].str.split().str.len() <= 5]  # Берём строки с количеством слов <= n \n",
    "\n",
    "refined_void_df = pd.DataFrame()\n",
    "for idx, row in initial_void_df.iterrows():\n",
    "\ttext = row['PreprocessedText'].split()  # Предобработанный текст\n",
    "\tfor key, keywords in create_dataset.topics.items():\n",
    "\t\t# Проверяем наличие ключевых слов в тексте с использованием нечеткого поиска\n",
    "\t\tif any(True for keyword in keywords for word in text if fuzz.ratio(word, keyword) > 67):\n",
    "\t\t\tbreak\n",
    "\telse:\n",
    "\t\t# Нет ключевых слов (нулевые классы)\n",
    "\t\trefined_void_df = pd.concat([refined_void_df, row.to_frame().T], ignore_index=True)\n",
    "refined_void_df[topic_keywords] = 0\n",
    "\n",
    "# Объединяем размеченные и нулевые отзывы\n",
    "refined_void_df = refined_void_df[:100]  # Берём только n шт. нулевых классов\n",
    "df_marked = pd.concat([df_marked, refined_void_df], ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_marked.shape",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Загрузка данных"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcsWUODBwScb"
   },
   "source": "Загрузим данные, которые мы будем использовать для обучения\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPqoVTjJwpDF"
   },
   "source": "Посмотрим на распределение классов в выборке"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aHyGuTFgyPO"
   },
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "При работе с моделью BERT предобработка в традиционном смысле (удаление стоп-слов, знаков препинания) не требуется.\n",
    "\n",
    "Нужна предобработка другого рода:\n",
    "* добавление спецтокенов для разделения предложений [SEP] и классификации [CLS]\n",
    "* приведение всех предложений к одинаковой длине (паддинг)\n",
    "* создание маски внимания (attention mask) — списка из 0 и 1, где 0 соответствует вспомогательным токенам (padding), а 1 — настоящим.\n",
    "\n",
    "Нам не нужно самим добавлять спецсимволы и составлять словарь соответствия токенов и индексов. Это сделает токенизатор, соотвествующий выбранной модели. Сегодня мы будем использовать модель 'bert-base-cased'."
   ],
   "metadata": {
    "id": "f7J6xgvK-bgq"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E7Mj-0ne--5t"
   },
   "source": "PRE_TRAINED_MODEL_NAME = 'DeepPavlov/rubert-base-cased'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H3AfJSZ8NNLF"
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, clean_up_tokenization_spaces=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfrSbwTQ-wi_"
   },
   "source": [
    "Вспомним, как работает модель токенизации для BERT.\n",
    "\n",
    "- Токенизируйте текст `sample_txt` и переведите токены `tokens` в индексы `token_ids`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DX_JNJpSDnP1"
   },
   "source": [
    "sample_txt = df_marked.loc[0, 'Reviews']\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f'Предложение: {sample_txt}')\n",
    "print(f'Токены: {tokens}')\n",
    "print(f'Индексы токенов: {token_ids}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Специальные токены"
   ],
   "metadata": {
    "id": "t6V6ua4QAmAt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Токенизатор уже содержит индексы для спецсимволов:\n",
    "- [SEP] — метка конца предложения\n",
    "- [CLS] — токен для классификации предложения\n",
    "- [PAD] — токен для выравнивания длин последовательностей"
   ],
   "metadata": {
    "id": "rvkGVWPv_h2E"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EXwz47bQvCbc"
   },
   "source": [
    "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9ap7jdL0LYU"
   },
   "source": [
    "Вся предобработка может быть сделана с помощью метода `encode_plus`. Он возвращает словарь с ключами `input_ids` и `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vea9edaaxSPO"
   },
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "\tsample_txt,  # преобразуемый текст\n",
    "\tmax_length=256,  # максимальная длина\n",
    "\tadd_special_tokens=True,  # добавить спецтокены [CLS] и [SEP]\n",
    "\treturn_token_type_ids=False,  # вернуть номер предложения\n",
    "\tpadding='max_length',  # паддинг по установленной максимальной длине\n",
    "\treturn_attention_mask=True,  # создать маску для механизма внимания\n",
    "\treturn_tensors='pt',  # вернуть тензор PyTorch\n",
    "\ttruncation=True  # обрезать предложения длинее max_length\n",
    ")\n",
    "encoding"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Унификация длины предложений"
   ],
   "metadata": {
    "id": "scsck0ftF5Vl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проанализируем, какая длина отзывов встречается в данных чаще. Отберем отзывы длины менее 512 токенов, поскольку это максимальная длина последовательности для модели BERT."
   ],
   "metadata": {
    "id": "IKlffrnlLzPE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "token_lens = []\n",
    "for txt in df_marked['Reviews'].tolist():\n",
    "\ttokens = tokenizer.encode(txt, max_length=512)\n",
    "\ttoken_lens.append(len(tokens))\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256])\n",
    "plt.xlabel('Token count')"
   ],
   "metadata": {
    "id": "P-kkQBgDGBDF"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC0YgbzcFvpZ"
   },
   "source": "Установим максимальную длину последовательности равной 250."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t7xSmJtLuoxW"
   },
   "source": "MAX_LEN = 250",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Создание датасета",
   "metadata": {
    "id": "hkGgjdv8Qw_p"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvvcoU6nurHy"
   },
   "source": [
    "Теперь создадим датасет PyTorch, который понадобится для обучения и тестирования модели.\n",
    "- В методе `__init__` задаем тексты отзывов (`self.reviews`), метки классов (`self.targets`), токенизатор (`self.tokenizer`) и максимальную длину последовательности (`self.max_len`).\n",
    "- В методе `__len__` определяем размер датасета.\n",
    "- В методе `__getitem__` сопоставляем тексты отзывов и метки классов по индексу (`item`). Метод возвращает словарь: текст отзыва, индексы токенов, маску внимания, метку класса.\n",
    "\n",
    "- Добавьте предобработку отзыва `review`: преобразуйте его в словарь `encoding` с помощью метода `encode_plus`, добавьте спецтокены, установите максимальную длину для паддинга, не возвращайте номер предложения, задайте паддинг по максимальной установленной длине, создайте маску для механизма внимания, установите формат списка индексов как тензор pytorch, установите усечение для предложений больше максимальной длины."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E2BPgRJ7YBK0"
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "\tdef __init__(self, reviews, targets, tokenizer, max_len):\n",
    "\t\tself.reviews = reviews\n",
    "\t\tself.targets = targets\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.max_len = max_len\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.reviews)\n",
    "\n",
    "\tdef __getitem__(self, item):\n",
    "\t\treview = str(self.reviews[item])\n",
    "\t\ttarget = self.targets[item]\n",
    "\n",
    "\t\tencoding = self.tokenizer.encode_plus(\n",
    "\t\t\treview,  # преобразуемый текст\n",
    "\t\t\tmax_length=MAX_LEN,  # максимальная длина\n",
    "\t\t\tadd_special_tokens=True,  # добавить спецтокены [CLS] и [SEP]\n",
    "\t\t\treturn_token_type_ids=False,  # вернуть номер предложения\n",
    "\t\t\tpadding='max_length',  # паддинг по установленной максимальной длине\n",
    "\t\t\treturn_attention_mask=True,  # создать маску для механизма внимания\n",
    "\t\t\treturn_tensors='pt',  # вернуть тензор PyTorch\n",
    "\t\t\ttruncation=True  # обрезать предложения длинее max_length\n",
    "\t\t)\n",
    "\t\treturn {\n",
    "\t\t\t'review_text': review,\n",
    "\t\t\t'input_ids': encoding['input_ids'].flatten(),\n",
    "\t\t\t'attention_mask': encoding['attention_mask'].flatten(),\n",
    "\t\t\t'targets': torch.tensor(target, dtype=torch.long)\n",
    "\t\t}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2uwsvCYqDJK"
   },
   "source": [
    "Разделим данные на обучающую, валидационную и тестовую выборки. 90% всех данных отберем для обучения, оставшиеся 10% поделим пополам для валидации и тестирования."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B-vWzoo81dvO"
   },
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "df_train, df_test = train_test_split(df_marked, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f'Размеры датасетов:')\n",
    "print(f'Обучающая выборка: {df_train.shape}')\n",
    "print(f'Валидационная выборка: {df_val.shape}')\n",
    "print(f'Тестовая выборка: {df_test.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Определяем устройство для выполнения вычислений (CPU или GPU)",
   "metadata": {
    "id": "28tlCr5WZVf6"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S7_ACkTsHxF8"
   },
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4tQ1x-vqNab"
   },
   "source": [
    "Создадим итераторы по данным:\n",
    "- `train_data_loader` — данные для дообучения модели;\n",
    "- `val_data_loader` — данные для валидации модели при обучении;\n",
    "- `test_data_loader` — данные для тестирования модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KEGqcvkuOuTX"
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def create_data_loader(df_marked, tokenizer, max_len, batch_size):\n",
    "\tds = GPReviewDataset(\n",
    "\t\treviews=df_marked['Reviews'].to_numpy(),\n",
    "\t\ttargets=df_marked[topic_keywords].to_numpy(),\n",
    "\t\t# укажите, какие целевые переменные хотите использовать\n",
    "\t\ttokenizer=tokenizer,\n",
    "\t\tmax_len=max_len\n",
    "\t)\n",
    "\n",
    "\treturn DataLoader(\n",
    "\t\tds,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tnum_workers=1\n",
    "\t)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6dlOptwqlhF"
   },
   "source": [
    "Посмотрим на пример одного батча из итератора `train_data_loader`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y93ldSN47FeT"
   },
   "source": [
    "data = next(iter(train_data_loader))\n",
    "print(f'Батч:\\n{data.keys()}\\n')\n",
    "print(f\"Предложения в батче:\\n{data['review_text']}\\n\")\n",
    "print(f\"Индексы токенов:\\n{data['input_ids'].shape}\\nbatch size x max len\\n\")\n",
    "print(f\"Маски внимания:\\n{data['attention_mask'].shape}\\nbatch size x max len\\n\")\n",
    "print(f\"Метки классов:\\n{data['targets']}\")\n",
    "print(data['targets'].shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H63Y-TjyRC7S"
   },
   "source": [
    "## Загрузка и создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "440Nd31VTHER"
   },
   "source": [
    "### Загрузка предобученной модели"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0P41FayISNRI"
   },
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "bert_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFE7YSbFdY4t"
   },
   "source": [
    "Попробуем использовать эту модель. Применим её к токенизированному предложению. Модель принимает индексы токенов и маску внимания.\n",
    "\n",
    "В переменную `hidden_states` записаны скрытые состояние слоя эмбеддингов и всех слоев энкодера (векторы каждого токена в предложении), в переменную `last_hidden_state` — скрытые состояния последнего слоя энкодера модели. Переменная `pooled_output` содержит выход линейного слоя модели — контекстный вектор для токена [CLS]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WCCJYiBPcxGF"
   },
   "source": [
    "print(f\"Токены:\\n{tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])}\")\n",
    "print(f\"Индексы токенов:\\n{encoding['input_ids'][0]}\")\n",
    "print(f\"Маска внимания:\\n{encoding['attention_mask']}\")\n",
    "\n",
    "last_hidden_state, pooled_output, hidden_states = bert_model(\n",
    "\tinput_ids=encoding['input_ids'],\n",
    "\tattention_mask=encoding['attention_mask'],\n",
    "\toutput_hidden_states=True,\n",
    "\treturn_dict=False)\n",
    "\n",
    "print(f'\\nВсего скрытых состояний: {len(hidden_states)}')\n",
    "print(f'Размер скрытых состояний последнего слоя: {last_hidden_state.shape}')\n",
    "print(f'Размер выхода линейного слоя: {pooled_output.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Анализ контекстных векторов"
   ],
   "metadata": {
    "id": "xX1wyEfFkjY3"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Посмотрим на векторы модели BERT для некоторых слов в предложениях."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# text1 = df_marked.loc[16]['Reviews']\n",
    "# text2 = df_marked.loc[338]['Reviews']\n",
    "# test_word1, test_word2, test_word3 = 'курс', 'любая', 'практики'\n",
    "# print(text1, len(text1))\n",
    "# print(text2, len(text2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TRWIeY_x4j1g"
   },
   "source": [
    "# encoding = tokenizer.encode_plus(\n",
    "# \ttext1,\n",
    "# \tmax_length=60,\n",
    "# \tadd_special_tokens=True,\n",
    "# \treturn_token_type_ids=False,\n",
    "# \tpadding='max_length',\n",
    "# \treturn_attention_mask=True,\n",
    "# \treturn_tensors='pt',\n",
    "# \ttruncation=True\n",
    "# )\n",
    "# print(f\"Токенизированное предложение: {tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])}\")\n",
    "# \n",
    "# position1 = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]).index(test_word1)\n",
    "# print(f'Позиция слова \"{test_word1}\": {position1}')\n",
    "# \n",
    "# last_hidden_state, pooled_output = bert_model(\n",
    "# \tinput_ids=encoding['input_ids'],\n",
    "# \tattention_mask=encoding['attention_mask'],\n",
    "# \treturn_dict=False)\n",
    "# emb1 = last_hidden_state[0, position1, :]\n",
    "# print(f'Размер вектора \"{test_word1}\": {emb1.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# encoding2 = tokenizer.encode_plus(\n",
    "# \ttext2,\n",
    "# \tmax_length=60,\n",
    "# \tadd_special_tokens=True,\n",
    "# \treturn_token_type_ids=False,\n",
    "# \tpadding='max_length',\n",
    "# \treturn_attention_mask=True,\n",
    "# \treturn_tensors='pt',\n",
    "# \ttruncation=True\n",
    "# )\n",
    "# \n",
    "# print(f\"Токенизированное предложение: {tokenizer.convert_ids_to_tokens(encoding2['input_ids'][0])}\")\n",
    "# position2 = tokenizer.convert_ids_to_tokens(encoding2['input_ids'][0]).index(test_word2)\n",
    "# print(f'Позиция слова \"{test_word2}\": {position2}')\n",
    "# position3 = tokenizer.convert_ids_to_tokens(encoding2['input_ids'][0]).index(test_word3)\n",
    "# print(f'Позиция слова \"{test_word3}\": {position3}')\n",
    "# \n",
    "# last_hidden_state2, pooled_output2 = bert_model(\n",
    "# \tinput_ids=encoding2['input_ids'],\n",
    "# \tattention_mask=encoding2['attention_mask'],\n",
    "# \treturn_dict=False)\n",
    "# \n",
    "# emb2 = last_hidden_state2[0, position2, :]\n",
    "# emb3 = last_hidden_state[0, position3, :]\n",
    "# print(f'Размер вектора \"{test_word2}\": {emb2.shape}')\n",
    "# print(f'Размер вектора \"{test_word3}\": {emb3.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посчитаем косинусное расстояние."
   ],
   "metadata": {
    "id": "wEL0gszKm8er"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ux9mPEW_4OUg"
   },
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# \n",
    "# print(\n",
    "# \tf'Косинусное расстояние между словами \"{test_word1}\" и \"{test_word2}\": {cosine_similarity([emb1.detach().cpu().numpy()], [emb2.detach().cpu().numpy()])[0][0]}')\n",
    "# print(\n",
    "# \tf'Косинусное расстояние между словами \"{test_word1}\" и \"{test_word3}\": {cosine_similarity([emb1.detach().cpu().numpy()], [emb3.detach().cpu().numpy()])[0][0]}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Создание модели для классификации"
   ],
   "metadata": {
    "id": "jeFZlc-HnBKo"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o_NiS3WgOFf"
   },
   "source": [
    "Создадим класс `SentimentClassifier` на основе модели BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m_mRflxPl32F"
   },
   "source": [
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class ReviewsClassifier(nn.Module):\n",
    "\n",
    "\tdef __init__(self, n_classes):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\t\tself.drop = nn.Dropout(p=0.3)\n",
    "\n",
    "\t\t# Создаем отдельный линейный слой для каждого класса\n",
    "\t\tself.classifiers = nn.ModuleList([\n",
    "\t\t\tnn.Linear(self.bert.config.hidden_size, 1) for _ in range(n_classes)\n",
    "\t\t])\n",
    "\n",
    "\tdef forward(self, input_ids, attention_mask):\n",
    "\t\t_, pooled_output = self.bert(\n",
    "\t\t\tinput_ids=input_ids,\n",
    "\t\t\tattention_mask=attention_mask,\n",
    "\t\t\treturn_dict=False)\n",
    "\n",
    "\t\toutput = self.drop(pooled_output)\n",
    "\n",
    "\t\t# Применяем каждый линейный слой к одному и тому же входу, но независимо\n",
    "\t\toutputs = [torch.sigmoid(classifier(output)) for classifier in self.classifiers]\n",
    "\n",
    "\t\t# Соединяем все предсказания в один тензор\n",
    "\t\treturn torch.cat(outputs, dim=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i0yQnuSFsjDp"
   },
   "source": [
    "n_classes = len(topic_keywords)\n",
    "model = ReviewsClassifier(n_classes)\n",
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCPCFDLlKIQd"
   },
   "source": [
    "Перенесем на видеокарту один из батчей для примера и применим модель к данным."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mz7p__CqdaMO"
   },
   "source": [
    "# input_ids = data['input_ids'].to(device)\n",
    "# attention_mask = data['attention_mask'].to(device)\n",
    "# \n",
    "# print(f'Индексы токенов:\\n{input_ids.shape}\\nbatch size x seq length')\n",
    "# print(f'Маска внимания:\\n{attention_mask.shape}\\nbatch size x seq length')\n",
    "# \n",
    "# output = model(input_ids, attention_mask)\n",
    "# print(f'\\nВыход модели:\\n{output}')\n",
    "# print(f'Размер:\\n{output.shape}\\nbatch size x num classes')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "## Обучение, валидация и тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обучение и валидация"
   ],
   "metadata": {
    "id": "mktqckaD3TX1"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76g7FV85H-T8"
   },
   "source": [
    "Для дообучения модели будем использовать оптимизатор AdaW из библиотеки Transformers.\n",
    "\n",
    "Авторы модели BERT рекомендуют использовать следующие параметры для дообучения модели:\n",
    "- Размер батча: 16, 32\n",
    "- Скорость обучения (с оптимизатором Adam): 5e-5, 3e-5, 2e-5\n",
    "- Количество эпох: 2, 3, 4\n",
    "\n",
    "Дообучение может происходить двумя способами:\n",
    "- меняются веса на всех слоях (`requires_grad=True`);\n",
    "- часть весов замораживается (`requires_grad=False`), для оставшихся слоев веса меняются (`requires_grad=True`).\n",
    "\n",
    "По умолчанию для всех весов `requires_grad=True`. Чтобы заморозить веса, нужно установить параметр `requires_grad=False`. Заморозим веса для первых 5 слоев энкодера."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'До заморозки:\\n {list(model.bert.encoder.layer[4].parameters())[0]}')\n",
    "\n",
    "for layer_id in range(5):\n",
    "\tfor param in list(model.bert.encoder.layer[layer_id].parameters()):\n",
    "\t\tparam.requires_grad = False\n",
    "\n",
    "print(f'\\nПосле заморозки:\\n {list(model.bert.encoder.layer[4].parameters())[0]}')"
   ],
   "metadata": {
    "id": "-7EsPF8j9yNG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Зададим количество эпох и скорость обучения. Будем использовать планировщик (`scheduler`), он регулирует скорость обучения: первые несколько шагов (`num_warmup_steps`) она может увеличиваться, а потом уменьшается. Также определим функцию потерь."
   ],
   "metadata": {
    "id": "6QMzE_3q0xBp"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5v-ArJ2fCCcU"
   },
   "source": [
    "import os\n",
    "\n",
    "EPOCHS = 1\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Обучение всех слоев\n",
    "#optimizer = AdamW(model.parameters(), lr=2e-5) # обучение всех слоев\n",
    "# Для обучения только незамороженных слоев нужно установить фильтр\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\toptimizer,\n",
    "\tnum_warmup_steps=0,\n",
    "\tnum_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.BCELoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8522g7JIu5J"
   },
   "source": [
    "Реализуем функцию для одной эпохи обучения."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bzl9UhuNx1_Q"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "\tmodel.train()\n",
    "\tlosses = []\n",
    "\tcorrect_predictions = 0\n",
    "\n",
    "\tfor batch in tqdm(data_loader):\n",
    "\t\tinput_ids = batch[\"input_ids\"].to(device)\n",
    "\t\tattention_mask = batch[\"attention_mask\"].to(device)\n",
    "\t\ttargets = batch[\"targets\"].to(device).float()  # Преобразование targets в float\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\toutputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\t\tloss = loss_fn(outputs, targets)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tscheduler.step()\n",
    "\n",
    "\t\t# Применяем порог 0.5 для бинаризации предсказаний\n",
    "\t\tpreds = (outputs > 0.5).int()\n",
    "\n",
    "\t\t# Сравниваем предсказания с целями по каждому классу\n",
    "\t\tcorrect_predictions += torch.sum(preds == targets).item()\n",
    "\t\tlosses.append(loss.item())\n",
    "\n",
    "\tavg_loss = np.mean(losses)\n",
    "\n",
    "\t# Количество всех предсказаний равно числу примеров, умноженному на количество классов\n",
    "\taccuracy = correct_predictions / (n_examples * targets.shape[1])\n",
    "\treturn accuracy, avg_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4PniYIte0fr"
   },
   "source": [
    "Также реализуем функцию для валидации."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CXeRorVGIKre"
   },
   "source": [
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "\tmodel.eval()\n",
    "\tlosses = []\n",
    "\tcorrect_predictions = 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in data_loader:\n",
    "\t\t\tinput_ids = batch[\"input_ids\"].to(device)\n",
    "\t\t\tattention_mask = batch[\"attention_mask\"].to(device)\n",
    "\t\t\ttargets = batch[\"targets\"].to(device).float()  # Преобразование targets в float\n",
    "\n",
    "\t\t\toutputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\t\t\tloss = loss_fn(outputs, targets)\n",
    "\n",
    "\t\t\t# Применяем порог 0.5 для бинаризации предсказаний\n",
    "\t\t\tpreds = (outputs > 0.5).int()\n",
    "\n",
    "\t\t\t# Сравниваем предсказания с целями по каждому классу\n",
    "\t\t\tcorrect_predictions += torch.sum(preds == targets).item()\n",
    "\n",
    "\t\t\tlosses.append(loss.item())\n",
    "\n",
    "\tavg_loss = np.mean(losses)\n",
    "\n",
    "\t# Количество всех предсказаний равно числу примеров, умноженному на количество классов\n",
    "\taccuracy = correct_predictions / (n_examples * targets.shape[1])\n",
    "\n",
    "\treturn accuracy, avg_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_rdSDBHhhCh"
   },
   "source": [
    "Используя эти две функции, реализуем процедуру дообучения модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xHbOmV_hJ5Ny"
   },
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "path = '../weights/saved_weights.pt'\n",
    "model = model.to(device)\n",
    "\n",
    "# Проверяем, существует ли директория и создаем её, если она не существует\n",
    "directory = os.path.dirname(path)\n",
    "if not os.path.exists(directory):\n",
    "\tos.makedirs(directory)\n",
    "\n",
    "if os.path.exists(path):\n",
    "\tprint(\"Файл с весами найден. Загружаем веса вместо обучения.\")\n",
    "\tmodel.load_state_dict(torch.load(path))\n",
    "\thistory = None\n",
    "else:\n",
    "\t# Если файла с весами нет, то обучаем модель\n",
    "\tfrom collections import defaultdict\n",
    "\n",
    "\thistory = defaultdict(list)\n",
    "\n",
    "\tfor epoch in range(EPOCHS):\n",
    "\t\tprint(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "\t\tprint('-' * 10)\n",
    "\n",
    "\t\ttrain_acc, train_loss = train_epoch(\n",
    "\t\t\tmodel,\n",
    "\t\t\ttrain_data_loader,\n",
    "\t\t\tloss_fn,\n",
    "\t\t\toptimizer,\n",
    "\t\t\tdevice,\n",
    "\t\t\tscheduler,\n",
    "\t\t\tlen(df_train)\n",
    "\t\t)\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\tprint(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\t\tval_acc, val_loss = eval_model(\n",
    "\t\t\tmodel,\n",
    "\t\t\tval_data_loader,\n",
    "\t\t\tloss_fn,\n",
    "\t\t\tdevice,\n",
    "\t\t\tlen(df_val)\n",
    "\t\t)\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\t\tprint(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "\n",
    "\t\thistory['train_acc'].append(train_acc)\n",
    "\t\thistory['train_loss'].append(train_loss)\n",
    "\t\thistory['val_acc'].append(val_acc)\n",
    "\t\thistory['val_loss'].append(val_loss)\n",
    "\n",
    "\ttorch.save(model.state_dict(), path)\n",
    "\ttorch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Можем сравнить точность на обучающей и валидационной выборке."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "60vAuburGsBk"
   },
   "source": [
    "if history is not None:  # Если модель обучалась, то можем сравнить точность эпох (не имеет смысла, если установлена 1 эпоха обучения) \n",
    "\timport matplotlib.pyplot as plt\n",
    "\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\n",
    "\t# График точности (accuracy)\n",
    "\tplt.subplot(1, 2, 1)\n",
    "\tplt.plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "\tplt.plot(history['val_acc'], label='Validation Accuracy', marker='o')\n",
    "\tplt.title('Accuracy Over Epochs')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Accuracy')\n",
    "\tplt.legend()\n",
    "\tplt.ylim([0, 1])\n",
    "\tplt.grid(True)\n",
    "\n",
    "\t# График потерь (loss)\n",
    "\tplt.subplot(1, 2, 2)\n",
    "\tplt.plot(history['train_loss'], label='Train Loss', marker='o', color='r')\n",
    "\tplt.plot(history['val_loss'], label='Validation Loss', marker='o', color='b')\n",
    "\tplt.title('Loss Over Epochs')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kU9MfvTQC9wh"
   },
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnaSaPXMCuBJ"
   },
   "source": [
    "Подсчитаем точность (accuracy) модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jS3gJ_qBEljD"
   },
   "source": [
    "test_acc, _ = eval_model(\n",
    "\tmodel,\n",
    "\ttest_data_loader,\n",
    "\tloss_fn,\n",
    "\tdevice,\n",
    "\tlen(df_test)\n",
    ")\n",
    "\n",
    "test_acc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WL5pDmvFyaU"
   },
   "source": [
    "### Предсказание на произвольных текстах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1cg2r4SvLrT"
   },
   "source": [
    "Теперь нам осталось научиться использовать нашу модель для предсказания класса любого текста.\n",
    "\n",
    " Придумайте свой отзыв и проверьте работу модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QEPi7zQRsDhH"
   },
   "source": "review_text = \"\"\"Этот курс оставил у меня только положительные впечатления благодаря тому, как были поданы материалы. Ведущий демонстрировал высокий уровень знаний, что помогало глубже понять сложные темы. Каждый новый раздел был структурирован логично, с понятными примерами и практическими заданиями. Особое внимание уделялось важным деталям, что значительно облегчало усвоение информации. Рекомендую этот курс всем.\"\"\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBSHv23kvXeO"
   },
   "source": [
    "Для использования нашей модели нам следует токенизировать текст соответствующим образом."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zA5Or4D2sLc9"
   },
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "\treview_text,\n",
    "\tmax_length=MAX_LEN,\n",
    "\tadd_special_tokens=True,\n",
    "\treturn_token_type_ids=False,\n",
    "\tpadding='max_length',\n",
    "\treturn_attention_mask=True,\n",
    "\treturn_tensors='pt',\n",
    "\ttruncation=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foCrgUbJvF1R"
   },
   "source": [
    "Теперь получим предсказания нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Ue9pEDKHmFo"
   },
   "source": [
    "input_ids = encoded_review['input_ids']\n",
    "attention_mask = encoded_review['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = model(input_ids.to(device), attention_mask.to(device))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Применяем пороговое значение для классификации\n",
    "predictions = (output > 0.5)\n",
    "predictions = predictions.to(torch.int)\n",
    "\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Prediction  : {predictions}')\n",
    "print('Предсказанные темы: {}'.format({topic_keywords[i] for i, val in enumerate(predictions.tolist()[0]) if val}))\n",
    "print(output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Функция для получения предсказания для одного отзыва\n",
    "def predict_review(review_text):\n",
    "\tencoded_review = tokenizer.encode_plus(\n",
    "\t\treview_text,\n",
    "\t\tmax_length=MAX_LEN,\n",
    "\t\tadd_special_tokens=True,\n",
    "\t\treturn_token_type_ids=False,\n",
    "\t\tpadding='max_length',\n",
    "\t\treturn_attention_mask=True,\n",
    "\t\treturn_tensors='pt',\n",
    "\t\ttruncation=True\n",
    "\t)\n",
    "\n",
    "\tinput_ids = encoded_review['input_ids'].to(device)\n",
    "\tattention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_ids, attention_mask)\n",
    "\n",
    "\ttorch.cuda.empty_cache()\n",
    "\n",
    "\tpredictions = (output > 0.5).to(torch.int)\n",
    "\n",
    "\treturn predictions.tolist()[0]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_t = pd.read_csv('../data/test_reviews.csv', index_col=0)\n",
    "\n",
    "# Проход по всему DataFrame и предсказание меток для каждого отзыва\n",
    "for index, row in df_t.iterrows():\n",
    "\treview_text = row['Reviews']\n",
    "\tpredictions = predict_review(review_text)\n",
    "\n",
    "\t# Записываем предсказания в соответствующие столбцы\n",
    "\tdf_t.at[index, 'практика'] = predictions[0]\n",
    "\tdf_t.at[index, 'теория'] = predictions[1]\n",
    "\tdf_t.at[index, 'преподаватель'] = predictions[2]\n",
    "\tdf_t.at[index, 'технологии'] = predictions[3]\n",
    "\tdf_t.at[index, 'актуальность'] = predictions[4]\n",
    "\n",
    "# Теперь DataFrame df_t содержит обновленные метки для каждого отзыва\n",
    "print(df_t)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_t.to_csv('../data/answer_bert.csv')",
   "outputs": [],
   "execution_count": null
  }
 ]
}
