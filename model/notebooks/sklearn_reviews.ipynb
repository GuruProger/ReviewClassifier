{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae92cc0f",
   "metadata": {},
   "source": "# Анализ и обработка отзывов"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pymorphy3\n",
    "\n",
    "# Инициализация морфологического анализатора для работы с русским языком\n",
    "morph_analyzer = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "\n",
    "class CreateDataset:\n",
    "\tdef __init__(self):\n",
    "\t\t# Определяем ключевые слова для категорий отзывов\n",
    "\t\tself.practice = {'задачи', 'кейс', 'проект', 'задания'}  # Слова, связанные с практикой\n",
    "\t\tself.theory = {'знания', 'лекция', 'материал'}  # Слова, связанные с теорией\n",
    "\t\tself.technology = {'проблема', 'ресурсы', 'инструменты', 'доступность',\n",
    "\t\t\t\t\t\t   'интерактивность'}  # Слова, связанные с технологиями\n",
    "\t\tself.teacher = {'лектор', 'учитель', 'профессор', 'наставник', 'педагог', 'препод', 'автор',\n",
    "\t\t\t\t\t\t'создатель'}  # Слова, связанные с преподавателем\n",
    "\t\tself.relevance = {'полезный', 'современный', 'релевантный', 'новый', 'старый',\n",
    "\t\t\t\t\t\t  'устарел'}  # Слова, связанные с актуальностью\n",
    "\n",
    "\t\t# Список ключевых тем\n",
    "\t\tself.topic_keywords = ['практика', 'теория', 'преподаватель', 'технологии', 'актуальность']\n",
    "\n",
    "\t\t# Создаем словарь, где ключевые темы связываются с набором соответствующих слов\n",
    "\t\tself.topics = {keyword: {keyword} for keyword in self.topic_keywords}\n",
    "\n",
    "\t\t# Обновляем словарь ключевых тем с учетом различных словоформ\n",
    "\t\tself.topics['практика'].update(self.practice)\n",
    "\t\tself.topics['теория'].update(self.theory)\n",
    "\t\tself.topics['преподаватель'].update(self.teacher)\n",
    "\t\tself.topics['технологии'].update(self.technology)\n",
    "\t\tself.topics['актуальность'].update(self.relevance)\n",
    "\n",
    "\tdef names_in_text(self, text: str) -> bool:\n",
    "\t\t\"\"\"\n",
    "        Проверяет, содержатся ли в тексте имена преподавателей.\n",
    "        :param text: Исходный текст\n",
    "        :return: True, если найдено хотя бы одно имя, иначе False\n",
    "        \"\"\"\n",
    "\t\tstop_names = frozenset('паскаль')  # Исключаемое имя (например, Паскаль)\n",
    "\t\tfor word in text:\n",
    "\t\t\tif word not in stop_names:\n",
    "\t\t\t\tparse = morph_analyzer.parse(word)[0]\n",
    "\t\t\t\ttag = parse.tag\n",
    "\t\t\t\t# Проверяем, является ли слово именем или фамилией с высокой вероятностью\n",
    "\t\t\t\tif ('Name' in tag or 'Surn' in tag) and parse.score >= 0.8:\n",
    "\t\t\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "\tdef automatic_annotation(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "\t\t\"\"\"\n",
    "        Автоматическая разметка обучающего набора данных на основе ключевых слов и имен преподавателей.\n",
    "        :param df: DataFrame с текстами для разметки\n",
    "        :return: Размеченный DataFrame\n",
    "        \"\"\"\n",
    "\t\t# Подготовка структуры для хранения данных\n",
    "\t\tdata_all = {t: [] for t in self.topic_keywords}\n",
    "\t\tdata_index = set()\n",
    "\n",
    "\t\t# Проходим по каждому тексту в DataFrame\n",
    "\t\tfor idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "\t\t\tpreprocessed_text = row['PreprocessedText']  # Предобработанный текст\n",
    "\t\t\tdata = []\n",
    "\t\t\tfor key, keywords in self.topics.items():\n",
    "\t\t\t\t# Проверяем наличие ключевых слов в тексте с использованием нечеткого поиска\n",
    "\t\t\t\tif any(True for keyword in keywords for word in preprocessed_text if fuzz.ratio(word, keyword) > 78):\n",
    "\t\t\t\t\tdata_index.add(idx)\n",
    "\t\t\t\t\tdata.append(1)\n",
    "\t\t\t\telif key == 'преподаватель' and self.names_in_text(preprocessed_text):\n",
    "\t\t\t\t\t# Дополнительная проверка на наличие имен преподавателей\n",
    "\t\t\t\t\tdata_index.add(idx)\n",
    "\t\t\t\t\tdata.append(1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Если текст не относится к категории\n",
    "\t\t\t\t\tdata.append(0)\n",
    "\t\t\tif sum(data) > 0:\n",
    "\t\t\t\t# Добавляем данные в соответствующие столбцы\n",
    "\t\t\t\tfor i, key in enumerate(self.topic_keywords):\n",
    "\t\t\t\t\tdata_all[key].append(data[i])\n",
    "\t\treturn pd.DataFrame(data_all, index=sorted(data_index))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обработка текста",
   "id": "b186dacdfc8834b5"
  },
  {
   "cell_type": "code",
   "id": "c93d3d6963232734",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Импортируем датасет с отзывами\n",
    "import pandas as pd\n",
    "from thefuzz import fuzz  # Новая версия библиотеки fuzzywuzzy\n",
    "import nltk\n",
    "\n",
    "# Загрузка необходОгромная благодарность авторам за труд! Полезный материал пра…имых данных для NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# from utils import CreateDataset\n",
    "\n",
    "# Загрузка стоп-слов\n",
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('../data/train_reviews.csv', index_col=0)\n",
    "df['Reviews'] = df['Reviews'].astype(str)\n",
    "\n",
    "# Замена кириллической 'с' на латинскую\n",
    "df['Reviews'] = df['Reviews'].apply(\n",
    "\tlambda x: x.replace('1с', '1c').replace('с#', 'c#').replace('с+', 'c+').replace('Ё', 'Е').replace(\n",
    "\t\t'1С', '1C').replace('С#', 'C#').replace('С+', 'C+'))\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "\tdef preprocess(self, text: str) -> list[str]:\n",
    "\t\t\"\"\"\n",
    "        Предобрабатывает текст, токенизируя, лемматизируя и удаляя стоп-слова.\n",
    "\n",
    "        :param text: Входной текст, который необходимо предобработать.\n",
    "        :return: Список предобработанных слов.\n",
    "        \"\"\"\n",
    "\t\ttokens = word_tokenize(text.lower())\n",
    "\t\tfiltered_tokens = [\n",
    "\t\t\tnormalized_word\n",
    "\t\t\tfor word in tokens\n",
    "\t\t\tif (normalized_word := morph_analyzer.parse(self._remove_symbols(word))[0].normal_form) not in stop_words\n",
    "\t\t]\n",
    "\n",
    "\t\tfiltered_tokens = \" \".join(filtered_tokens).replace('ё', 'е').split()  # Удаление лишних пробелов\n",
    "\t\treturn filtered_tokens\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _remove_symbols(text: str) -> str:\n",
    "\t\t\"\"\"Удаляет все символы и цифры из строки.\"\"\"\n",
    "\t\tclean_text = \"\".join(char if char.isalnum() and not char.isdigit() else \" \" for char in text)\n",
    "\t\treturn clean_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e40ce0cf9bd2c856",
   "metadata": {},
   "source": [
    "text_processor = TextProcessor()\n",
    "\n",
    "# Применение предобработки к отзывам\n",
    "df['PreprocessedText'] = df['Reviews'].apply(text_processor.preprocess)\n",
    "\n",
    "# Создание списка всех стеммированных слов\n",
    "all_words = [word for sublist in df['PreprocessedText'] for word in sublist]\n",
    "unique_words = pd.Series(all_words).value_counts()\n",
    "\n",
    "# Создание DataFrame с размеченными данными\n",
    "create_dataset = CreateDataset()\n",
    "df_dataset_index = create_dataset.automatic_annotation(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f81027e581040957",
   "metadata": {},
   "source": "Просмотр размеченных текстов"
  },
  {
   "cell_type": "code",
   "id": "aeda3f20b86fc6a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Всего размеченных текстов -', len(df_dataset_index))\n",
    "for t in create_dataset.topic_keywords:\n",
    "\tprint(t, '-', len(df_dataset_index[df_dataset_index[t] == 1]))\n",
    "\n",
    "# Создание списков для тем и их количества\n",
    "topics = create_dataset.topic_keywords\n",
    "counts = [len(df_dataset_index[df_dataset_index[t] == 1]) for t in topics]\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(topics, counts, color='skyblue')\n",
    "plt.xlabel('Темы')\n",
    "plt.ylabel('Количество размеченных текстов')\n",
    "plt.title('Количество размеченных текстов по темам')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c03a95458618dbc4",
   "metadata": {},
   "source": [
    "Размеченный и обработанный датасет"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a8167a8f531ce30",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Размечиваем и обрабатываем датасет\n",
    "df_marked: pd.DataFrame = df.loc[df_dataset_index.index]\n",
    "for topic in create_dataset.topic_keywords:\n",
    "\tdf_marked[topic] = df_dataset_index[topic]\n",
    "df_marked['PreprocessedText'] = df_marked['PreprocessedText'].apply(lambda x: \" \".join(x))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "933768d9fe67e0e6",
   "metadata": {},
   "source": "# Векторизация текста и обучение модели"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Векторизация",
   "id": "870df143a00782cf"
  },
  {
   "cell_type": "code",
   "id": "5f88f6839a924151",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Размечиваем и обрабатываем датасет\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Векторизация текста\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(df_marked['PreprocessedText'])\n",
    "y = df_marked.drop(['PreprocessedText', 'Reviews'], axis=1)\n",
    "# Разделим данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, df_marked[create_dataset.topic_keywords], test_size=0.3,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=42)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ef5a170d32633a7",
   "metadata": {},
   "source": "## Обучение модели"
  },
  {
   "cell_type": "code",
   "id": "de4a685c5988f38b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Обучаем модель классификации отзывов\n",
    "# Используем многоцелевой классификатор\n",
    "model = MultiOutputClassifier(DecisionTreeClassifier())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозируем на тестовых данных\n",
    "y_pred = model.predict(X_test)\n",
    "# Оценка модели\n",
    "print(classification_report(y_test, y_pred, target_names=y.columns, zero_division=0))\n",
    "print(model.score(X_test, y_test))\n",
    "print(f1_score(y_test, y_pred, average='macro'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5934779cf1cf1f14",
   "metadata": {},
   "source": "### Получим важность признаков\n"
  },
  {
   "cell_type": "code",
   "id": "9feec516b72012b5",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "for i, name in enumerate(list(y.columns)):\n",
    "\timportances = model.estimators_[i].feature_importances_\n",
    "\t\n",
    "\t# Отсортируем важность\n",
    "\tindices = np.argsort(importances)[::-1]\n",
    "\t\n",
    "\t# Напечатаем топ важнейших признаков\n",
    "\tfeature_names = vectorizer.get_feature_names_out()\n",
    "\ttop_n = 7\n",
    "\tprint('-'*32)\n",
    "\tprint(f\"Топ {top_n} важнейших слов для категории '{name}':\")\n",
    "\tfor i in range(top_n):\n",
    "\t\tif importances[indices[i]]:\n",
    "\t\t\tprint(f\"{feature_names[indices[i]]}: {importances[indices[i]]}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4b1c156ce727c14",
   "metadata": {},
   "source": "### Проверка тестового текста"
  },
  {
   "cell_type": "code",
   "id": "bcb4b962076ab4a2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test_review = \"Крутой курс! Мне понравились задачи и препод\"\n",
    "test_review = (text_processor.preprocess(test_review))\n",
    "test_review = \" \".join(test_review)\n",
    "new_vector = vectorizer.transform(pd.Series(test_review))\n",
    "test_pred = model.predict(new_vector)\n",
    "for i, k in enumerate(create_dataset.topic_keywords):\n",
    "\tprint(k, ':', test_pred[0][i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Создание ответов",
   "id": "74087082bffad77e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cоздание ответов с помощью модели",
   "id": "d6837cb73a77b0df"
  },
  {
   "cell_type": "code",
   "id": "f007ce9c39e4fbf9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_test = pd.read_csv('../data/test_reviews.csv', index_col=0)\n",
    "df_test['Reviews'] = df_test['Reviews'].astype(str)\n",
    "df_test['Pred'] = df_test['Reviews'].apply(lambda x: \" \".join(text_processor.preprocess(x)))\n",
    "test_vector = vectorizer.transform(df_test['Pred'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32dcd192a114e068",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test_predict = pd.DataFrame(model.predict(test_vector))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e80d6f86ebdbf5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i, k in enumerate(create_dataset.topic_keywords):\n",
    "\tdf_test[k] = test_predict[i]\n",
    "df_pred = df_test.drop('Pred', axis=1)\n",
    "df_pred.to_csv('../data/model_answer.csv', index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "83ee620ed5bc1c7d",
   "metadata": {},
   "source": "## Разметка при помощи алгоритма"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Добавим 'пустую' разметку",
   "id": "9b6a41fa6f0fec3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Пустые классы делают обучение bert более качественным\n",
    "void_df = df_test[(df_test['Pred'].str.split().str.len() <= 5) &\n",
    "\t\t\t\t (df_test['практика'] == 0) &\n",
    "\t\t\t\t (df_test['теория'] == 0) &\n",
    "\t\t\t\t (df_test['преподаватель'] == 0) &\n",
    "\t\t\t\t (df_test['технологии'] == 0) &\n",
    "\t\t\t\t (df_test['актуальность'] == 0)]\n",
    "void_df = void_df[:50]\n",
    "void_df['PreprocessedText'] = void_df['Pred']\n",
    "void_df = void_df.drop('Pred', axis=1)"
   ],
   "id": "19e1e266e42f1ce8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Размечиваем и рандомно перемешиваем датасет\n",
    "df_auto = pd.concat([void_df.drop('PreprocessedText', axis=1), df_marked.drop('PreprocessedText', axis=1)], axis=0,\n",
    "\t\t\t\t   ignore_index=True)\n",
    "df_auto = df_auto.sample(frac=1).reset_index(drop=True)\n",
    "df_auto.to_csv('../data/train_reviews_bert.csv', index=False) # Можно использовать для обучения bert"
   ],
   "id": "2ccca329732d1ec9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Комбинируем предсказания модели и автоматической разметки (логическое 'или')",
   "id": "6858b41c5534a81c"
  },
  {
   "cell_type": "code",
   "id": "86fd03ecdf70c5b4",
   "metadata": {},
   "source": [
    "# Предсказываем категории отзывов на основе обученной модели\n",
    "df_comb = df_pred.copy()\n",
    "df_comb['практика']  |= df_test['практика']\n",
    "df_comb['теория'] |= df_test['теория']\n",
    "df_comb['преподаватель'] |= df_test['преподаватель']\n",
    "df_comb['технологии'] |= df_test['технологии']\n",
    "df_comb['актуальность'] |= df_test['актуальность']\n",
    "\n",
    "df_comb.to_csv('../data/combined_answer.csv')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
