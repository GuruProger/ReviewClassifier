{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1xtYpR5L4Nsc_GBNBN_k5Vq-0PhojtEw7",
     "timestamp": 1684426980748
    },
    {
     "file_id": "1xiMJDWatyC_Lb2YkIEX0uHq2j1QHGx-q",
     "timestamp": 1684307916698
    }
   ],
   "collapsed_sections": [
    "ufzPdoTtNikq",
    "9aHyGuTFgyPO",
    "t6V6ua4QAmAt",
    "scsck0ftF5Vl",
    "hkGgjdv8Qw_p",
    "H63Y-TjyRC7S",
    "440Nd31VTHER",
    "xX1wyEfFkjY3",
    "jeFZlc-HnBKo",
    "g9xikRdtRN1N",
    "mktqckaD3TX1",
    "kU9MfvTQC9wh",
    "7WL5pDmvFyaU"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Загрузка данных"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcsWUODBwScb"
   },
   "source": "Загрузим данные, которые мы будем использовать для обучения\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mUKLyKc7I6Qp",
    "ExecuteTime": {
     "end_time": "2024-08-22T11:09:58.530432Z",
     "start_time": "2024-08-22T11:09:58.469678Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train_reviews_bert.csv\")\n",
    "df['Reviews'] = df['Reviews'].astype(str)\n",
    "# df = df.drop('PreprocessedText', axis=1)\n",
    "# df = df.reset_index(drop=True)\n",
    "df.head(6)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             Reviews  практика  теория  \\\n",
       "0  Благодраю за прекрасно подобранный курс.Доволь...         0       1   \n",
       "1  Нереально крутой курс !!!!! Нет \"воды\" в курсе...         1       1   \n",
       "2  Хороший курс, знакомит с концепциями постепенн...         0       1   \n",
       "3  Отличный курс. Спасибо авторам за проделанную ...         0       0   \n",
       "4  Очень зоркий спикер с прекрасной дикцией. Каса...         0       0   \n",
       "5  Спасибо большое команде курса! И, конечно, огр...         1       1   \n",
       "\n",
       "   преподаватель  технологии  актуальность  \n",
       "0              0           0             0  \n",
       "1              0           0             0  \n",
       "2              0           0             0  \n",
       "3              1           0             0  \n",
       "4              0           1             0  \n",
       "5              0           0             0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>практика</th>\n",
       "      <th>теория</th>\n",
       "      <th>преподаватель</th>\n",
       "      <th>технологии</th>\n",
       "      <th>актуальность</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Благодраю за прекрасно подобранный курс.Доволь...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Нереально крутой курс !!!!! Нет \"воды\" в курсе...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Хороший курс, знакомит с концепциями постепенн...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Отличный курс. Спасибо авторам за проделанную ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Очень зоркий спикер с прекрасной дикцией. Каса...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Спасибо большое команде курса! И, конечно, огр...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "df.shape",
   "metadata": {
    "id": "KMxWso521k5D"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPqoVTjJwpDF"
   },
   "source": "Посмотрим на распределение классов в выборке"
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topic_keywords = list(df.columns[1:])\n",
    "\n",
    "# Создание списков для тем и их количества\n",
    "counts = [len(df[df[t] == 1]) for t in topic_keywords]\n",
    "\n",
    "# Визуализация\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(topic_keywords, counts, color='skyblue')\n",
    "plt.xlabel('Темы')\n",
    "plt.ylabel('Количество размеченных текстов')\n",
    "plt.title('Количество размеченных текстов по темам')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "4VDZXoYNqmyT"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aHyGuTFgyPO"
   },
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "При работе с моделью BERT предобработка в традиционном смысле (удаление стоп-слов, знаков препинания) не требуется.\n",
    "\n",
    "Нужна предобработка другого рода:\n",
    "* добавление спецтокенов для разделения предложений [SEP] и классификации [CLS]\n",
    "* приведение всех предложений к одинаковой длине (паддинг)\n",
    "* создание маски внимания (attention mask) — списка из 0 и 1, где 0 соответствует вспомогательным токенам (padding), а 1 — настоящим.\n",
    "\n",
    "Нам не нужно самим добавлять спецсимволы и составлять словарь соответствия токенов и индексов. Это сделает токенизатор, соотвествующий выбранной модели. Сегодня мы будем использовать модель 'bert-base-cased'."
   ],
   "metadata": {
    "id": "f7J6xgvK-bgq"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E7Mj-0ne--5t"
   },
   "source": "PRE_TRAINED_MODEL_NAME = 'DeepPavlov/rubert-base-cased'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H3AfJSZ8NNLF"
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, clean_up_tokenization_spaces=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfrSbwTQ-wi_"
   },
   "source": [
    "Вспомним, как работает модель токенизации для BERT.\n",
    "\n",
    "- Токенизируйте текст `sample_txt` и переведите токены `tokens` в индексы `token_ids`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DX_JNJpSDnP1"
   },
   "source": [
    "sample_txt = df.loc[0, 'Reviews']\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f'Предложение: {sample_txt}')\n",
    "print(f'Токены: {tokens}')\n",
    "print(f'Индексы токенов: {token_ids}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Специальные токены"
   ],
   "metadata": {
    "id": "t6V6ua4QAmAt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Токенизатор уже содержит индексы для спецсимволов:\n",
    "- [SEP] — метка конца предложения\n",
    "- [CLS] — токен для классификации предложения\n",
    "- [PAD] — токен для выравнивания длин последовательностей"
   ],
   "metadata": {
    "id": "rvkGVWPv_h2E"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EXwz47bQvCbc"
   },
   "source": [
    "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9ap7jdL0LYU"
   },
   "source": [
    "Вся предобработка может быть сделана с помощью метода `encode_plus`. Он возвращает словарь с ключами `input_ids` и `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vea9edaaxSPO"
   },
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "\tsample_txt,  # преобразуемый текст\n",
    "\tmax_length=256,  # максимальная длина\n",
    "\tadd_special_tokens=True,  # добавить спецтокены [CLS] и [SEP]\n",
    "\treturn_token_type_ids=False,  # вернуть номер предложения\n",
    "\tpadding='max_length',  # паддинг по установленной максимальной длине\n",
    "\treturn_attention_mask=True,  # создать маску для механизма внимания\n",
    "\treturn_tensors='pt',  # вернуть тензор PyTorch\n",
    "\ttruncation=True  # обрезать предложения длинее max_length\n",
    ")\n",
    "encoding"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Унификация длины предложений"
   ],
   "metadata": {
    "id": "scsck0ftF5Vl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проанализируем, какая длина отзывов встречается в данных чаще. Отберем отзывы длины менее 512 токенов, поскольку это максимальная длина последовательности для модели BERT."
   ],
   "metadata": {
    "id": "IKlffrnlLzPE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "token_lens = []\n",
    "for txt in df['Reviews'].tolist():\n",
    "\ttokens = tokenizer.encode(txt, max_length=512)\n",
    "\ttoken_lens.append(len(tokens))\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256]);\n",
    "plt.xlabel('Token count');"
   ],
   "metadata": {
    "id": "P-kkQBgDGBDF"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC0YgbzcFvpZ"
   },
   "source": "Установим максимальную длину последовательности равной 250."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t7xSmJtLuoxW"
   },
   "source": "MAX_LEN = 250",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Создание датасета"
   ],
   "metadata": {
    "id": "hkGgjdv8Qw_p"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvvcoU6nurHy"
   },
   "source": [
    "Теперь создадим датасет PyTorch, который понадобится для обучения и тестирования модели.\n",
    "- В методе `__init__` задаем тексты отзывов (`self.reviews`), метки классов (`self.targets`), токенизатор (`self.tokenizer`) и максимальную длину последовательности (`self.max_len`).\n",
    "- В методе `__len__` определяем размер датасета.\n",
    "- В методе `__getitem__` сопоставляем тексты отзывов и метки классов по индексу (`item`). Метод возвращает словарь: текст отзыва, индексы токенов, маску внимания, метку класса.\n",
    "\n",
    "- Добавьте предобработку отзыва `review`: преобразуйте его в словарь `encoding` с помощью метода `encode_plus`, добавьте спецтокены, установите максимальную длину для паддинга, не возвращайте номер предложения, задайте паддинг по максимальной установленной длине, создайте маску для механизма внимания, установите формат списка индексов как тензор pytorch, установите усечение для предложений больше максимальной длины."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E2BPgRJ7YBK0"
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "\tdef __init__(self, reviews, targets, tokenizer, max_len):\n",
    "\t\tself.reviews = reviews\n",
    "\t\tself.targets = targets\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.max_len = max_len\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.reviews)\n",
    "\n",
    "\tdef __getitem__(self, item):\n",
    "\t\treview = str(self.reviews[item])\n",
    "\t\ttarget = self.targets[item]\n",
    "\n",
    "\t\tencoding = self.tokenizer.encode_plus(\n",
    "\t\t\treview,  # преобразуемый текст\n",
    "\t\t\tmax_length=MAX_LEN,  # максимальная длина\n",
    "\t\t\tadd_special_tokens=True,  # добавить спецтокены [CLS] и [SEP]\n",
    "\t\t\treturn_token_type_ids=False,  # вернуть номер предложения\n",
    "\t\t\tpadding='max_length',  # паддинг по установленной максимальной длине\n",
    "\t\t\treturn_attention_mask=True,  # создать маску для механизма внимания\n",
    "\t\t\treturn_tensors='pt',  # вернуть тензор PyTorch\n",
    "\t\t\ttruncation=True  # обрезать предложения длинее max_length\n",
    "\t\t)\n",
    "\t\treturn {\n",
    "\t\t\t'review_text': review,\n",
    "\t\t\t'input_ids': encoding['input_ids'].flatten(),\n",
    "\t\t\t'attention_mask': encoding['attention_mask'].flatten(),\n",
    "\t\t\t'targets': torch.tensor(target, dtype=torch.long)\n",
    "\t\t}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2uwsvCYqDJK"
   },
   "source": [
    "Разделим данные на обучающую, валидационную и тестовую выборки. 90% всех данных отберем для обучения, оставшиеся 10% поделим пополам для валидации и тестирования."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B-vWzoo81dvO"
   },
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f'Размеры датасетов:')\n",
    "print(f'Обучающая выборка: {df_train.shape}')\n",
    "print(f'Валидационная выборка: {df_val.shape}')\n",
    "print(f'Тестовая выборка: {df_test.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Определяем устройство для выполнения вычислений (CPU или GPU)",
   "metadata": {
    "id": "28tlCr5WZVf6"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S7_ACkTsHxF8"
   },
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4tQ1x-vqNab"
   },
   "source": [
    "Создадим итераторы по данным:\n",
    "- `train_data_loader` — данные для дообучения модели;\n",
    "- `val_data_loader` — данные для валидации модели при обучении;\n",
    "- `test_data_loader` — данные для тестирования модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KEGqcvkuOuTX"
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "\tds = GPReviewDataset(\n",
    "\t\treviews=df['Reviews'].to_numpy(),\n",
    "\t\ttargets=df[topic_keywords].to_numpy(),\n",
    "\t\t# укажите, какие целевые переменные хотите использовать\n",
    "\t\ttokenizer=tokenizer,\n",
    "\t\tmax_len=max_len\n",
    "\t)\n",
    "\n",
    "\treturn DataLoader(\n",
    "\t\tds,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tnum_workers=1\n",
    "\t)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6dlOptwqlhF"
   },
   "source": [
    "Посмотрим на пример одного батча из итератора `train_data_loader`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y93ldSN47FeT"
   },
   "source": [
    "data = next(iter(train_data_loader))\n",
    "print(f'Батч:\\n{data.keys()}\\n')\n",
    "print(f\"Предложения в батче:\\n{data['review_text']}\\n\")\n",
    "print(f\"Индексы токенов:\\n{data['input_ids'].shape}\\nbatch size x max len\\n\")\n",
    "print(f\"Маски внимания:\\n{data['attention_mask'].shape}\\nbatch size x max len\\n\")\n",
    "print(f\"Метки классов:\\n{data['targets']}\")\n",
    "print(data['targets'].shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H63Y-TjyRC7S"
   },
   "source": [
    "## Загрузка и создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "440Nd31VTHER"
   },
   "source": [
    "### Загрузка предобученной модели"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0P41FayISNRI"
   },
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "bert_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFE7YSbFdY4t"
   },
   "source": [
    "Попробуем использовать эту модель. Применим её к токенизированному предложению. Модель принимает индексы токенов и маску внимания.\n",
    "\n",
    "В переменную `hidden_states` записаны скрытые состояние слоя эмбеддингов и всех слоев энкодера (векторы каждого токена в предложении), в переменную `last_hidden_state` — скрытые состояния последнего слоя энкодера модели. Переменная `pooled_output` содержит выход линейного слоя модели — контекстный вектор для токена [CLS]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WCCJYiBPcxGF"
   },
   "source": [
    "print(f\"Токены:\\n{tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])}\")\n",
    "print(f\"Индексы токенов:\\n{encoding['input_ids'][0]}\")\n",
    "print(f\"Маска внимания:\\n{encoding['attention_mask']}\")\n",
    "\n",
    "last_hidden_state, pooled_output, hidden_states = bert_model(\n",
    "\tinput_ids=encoding['input_ids'],\n",
    "\tattention_mask=encoding['attention_mask'],\n",
    "\toutput_hidden_states=True,\n",
    "\treturn_dict=False)\n",
    "\n",
    "print(f'\\nВсего скрытых состояний: {len(hidden_states)}')\n",
    "print(f'Размер скрытых состояний последнего слоя: {last_hidden_state.shape}')\n",
    "print(f'Размер выхода линейного слоя: {pooled_output.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Анализ контекстных векторов"
   ],
   "metadata": {
    "id": "xX1wyEfFkjY3"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Посмотрим на векторы модели BERT для некоторых слов в предложениях."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# text1 = df.loc[16]['Reviews']\n",
    "# text2 = df.loc[338]['Reviews']\n",
    "# test_word1, test_word2, test_word3 = 'курс', 'любая', 'практики'\n",
    "# print(text1, len(text1))\n",
    "# print(text2, len(text2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TRWIeY_x4j1g"
   },
   "source": [
    "# encoding = tokenizer.encode_plus(\n",
    "# \ttext1,\n",
    "# \tmax_length=60,\n",
    "# \tadd_special_tokens=True,\n",
    "# \treturn_token_type_ids=False,\n",
    "# \tpadding='max_length',\n",
    "# \treturn_attention_mask=True,\n",
    "# \treturn_tensors='pt',\n",
    "# \ttruncation=True\n",
    "# )\n",
    "# print(f\"Токенизированное предложение: {tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])}\")\n",
    "# \n",
    "# position1 = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]).index(test_word1)\n",
    "# print(f'Позиция слова \"{test_word1}\": {position1}')\n",
    "# \n",
    "# last_hidden_state, pooled_output = bert_model(\n",
    "# \tinput_ids=encoding['input_ids'],\n",
    "# \tattention_mask=encoding['attention_mask'],\n",
    "# \treturn_dict=False)\n",
    "# emb1 = last_hidden_state[0, position1, :]\n",
    "# print(f'Размер вектора \"{test_word1}\": {emb1.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# encoding2 = tokenizer.encode_plus(\n",
    "# \ttext2,\n",
    "# \tmax_length=60,\n",
    "# \tadd_special_tokens=True,\n",
    "# \treturn_token_type_ids=False,\n",
    "# \tpadding='max_length',\n",
    "# \treturn_attention_mask=True,\n",
    "# \treturn_tensors='pt',\n",
    "# \ttruncation=True\n",
    "# )\n",
    "# \n",
    "# print(f\"Токенизированное предложение: {tokenizer.convert_ids_to_tokens(encoding2['input_ids'][0])}\")\n",
    "# position2 = tokenizer.convert_ids_to_tokens(encoding2['input_ids'][0]).index(test_word2)\n",
    "# print(f'Позиция слова \"{test_word2}\": {position2}')\n",
    "# position3 = tokenizer.convert_ids_to_tokens(encoding2['input_ids'][0]).index(test_word3)\n",
    "# print(f'Позиция слова \"{test_word3}\": {position3}')\n",
    "# \n",
    "# last_hidden_state2, pooled_output2 = bert_model(\n",
    "# \tinput_ids=encoding2['input_ids'],\n",
    "# \tattention_mask=encoding2['attention_mask'],\n",
    "# \treturn_dict=False)\n",
    "# \n",
    "# emb2 = last_hidden_state2[0, position2, :]\n",
    "# emb3 = last_hidden_state[0, position3, :]\n",
    "# print(f'Размер вектора \"{test_word2}\": {emb2.shape}')\n",
    "# print(f'Размер вектора \"{test_word3}\": {emb3.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посчитаем косинусное расстояние."
   ],
   "metadata": {
    "id": "wEL0gszKm8er"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ux9mPEW_4OUg"
   },
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# \n",
    "# print(\n",
    "# \tf'Косинусное расстояние между словами \"{test_word1}\" и \"{test_word2}\": {cosine_similarity([emb1.detach().cpu().numpy()], [emb2.detach().cpu().numpy()])[0][0]}')\n",
    "# print(\n",
    "# \tf'Косинусное расстояние между словами \"{test_word1}\" и \"{test_word3}\": {cosine_similarity([emb1.detach().cpu().numpy()], [emb3.detach().cpu().numpy()])[0][0]}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Создание модели для классификации"
   ],
   "metadata": {
    "id": "jeFZlc-HnBKo"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o_NiS3WgOFf"
   },
   "source": [
    "Создадим класс `SentimentClassifier` на основе модели BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m_mRflxPl32F"
   },
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "class ReviewsClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Создаем отдельный линейный слой для каждого класса\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.Linear(self.bert.config.hidden_size, 1) for _ in range(n_classes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False)\n",
    "        \n",
    "        output = self.drop(pooled_output)\n",
    "        \n",
    "        # Применяем каждый линейный слой к одному и тому же входу, но независимо\n",
    "        outputs = [torch.sigmoid(classifier(output)) for classifier in self.classifiers]\n",
    "        \n",
    "        # Соединяем все предсказания в один тензор\n",
    "        return torch.cat(outputs, dim=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i0yQnuSFsjDp"
   },
   "source": [
    "n_classes = len(topic_keywords)\n",
    "model = ReviewsClassifier(n_classes)\n",
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCPCFDLlKIQd"
   },
   "source": [
    "Перенесем на видеокарту один из батчей для примера и применим модель к данным."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mz7p__CqdaMO"
   },
   "source": [
    "# input_ids = data['input_ids'].to(device)\n",
    "# attention_mask = data['attention_mask'].to(device)\n",
    "# \n",
    "# print(f'Индексы токенов:\\n{input_ids.shape}\\nbatch size x seq length')\n",
    "# print(f'Маска внимания:\\n{attention_mask.shape}\\nbatch size x seq length')\n",
    "# \n",
    "# output = model(input_ids, attention_mask)\n",
    "# print(f'\\nВыход модели:\\n{output}')\n",
    "# print(f'Размер:\\n{output.shape}\\nbatch size x num classes')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "## Обучение, валидация и тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обучение и валидация"
   ],
   "metadata": {
    "id": "mktqckaD3TX1"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76g7FV85H-T8"
   },
   "source": [
    "Для дообучения модели будем использовать оптимизатор AdaW из библиотеки Transformers.\n",
    "\n",
    "Авторы модели BERT рекомендуют использовать следующие параметры для дообучения модели:\n",
    "- Размер батча: 16, 32\n",
    "- Скорость обучения (с оптимизатором Adam): 5e-5, 3e-5, 2e-5\n",
    "- Количество эпох: 2, 3, 4\n",
    "\n",
    "Дообучение может происходить двумя способами:\n",
    "- меняются веса на всех слоях (`requires_grad=True`);\n",
    "- часть весов замораживается (`requires_grad=False`), для оставшихся слоев веса меняются (`requires_grad=True`).\n",
    "\n",
    "По умолчанию для всех весов `requires_grad=True`. Чтобы заморозить веса, нужно установить параметр `requires_grad=False`. Заморозим веса для первых 5 слоев энкодера."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'До заморозки:\\n {list(model.bert.encoder.layer[4].parameters())[0]}')\n",
    "\n",
    "for layer_id in range(5):\n",
    "\tfor param in list(model.bert.encoder.layer[layer_id].parameters()):\n",
    "\t\tparam.requires_grad = False\n",
    "\n",
    "print(f'\\nПосле заморозки:\\n {list(model.bert.encoder.layer[4].parameters())[0]}')"
   ],
   "metadata": {
    "id": "-7EsPF8j9yNG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Зададим количество эпох и скорость обучения. Будем использовать планировщик (`scheduler`), он регулирует скорость обучения: первые несколько шагов (`num_warmup_steps`) она может увеличиваться, а потом уменьшается. Также определим функцию потерь."
   ],
   "metadata": {
    "id": "6QMzE_3q0xBp"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5v-ArJ2fCCcU"
   },
   "source": [
    "import os\n",
    "\n",
    "EPOCHS = 2\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Обучение всех слоев\n",
    "#optimizer = AdamW(model.parameters(), lr=2e-5) # обучение всех слоев\n",
    "# Для обучения только незамороженных слоев нужно установить фильтр\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\toptimizer,\n",
    "\tnum_warmup_steps=0,\n",
    "\tnum_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.BCELoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8522g7JIu5J"
   },
   "source": [
    "Реализуем функцию для одной эпохи обучения."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bzl9UhuNx1_Q"
   },
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "\tmodel.train()\n",
    "\tlosses = []\n",
    "\tcorrect_predictions = 0\n",
    "\n",
    "\tfor batch in tqdm(data_loader):\n",
    "\t\tinput_ids = batch[\"input_ids\"].to(device)\n",
    "\t\tattention_mask = batch[\"attention_mask\"].to(device)\n",
    "\t\ttargets = batch[\"targets\"].to(device).float()  # Преобразование targets в float\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\toutputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\t\tloss = loss_fn(outputs, targets)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tscheduler.step()\n",
    "\n",
    "\t\t# Применяем порог 0.5 для бинаризации предсказаний\n",
    "\t\tpreds = (outputs > 0.5).int()\n",
    "\n",
    "\t\t# Сравниваем предсказания с целями по каждому классу\n",
    "\t\tcorrect_predictions += torch.sum(preds == targets).item()\n",
    "\t\tlosses.append(loss.item())\n",
    "\n",
    "\tavg_loss = np.mean(losses)\n",
    "\n",
    "\t# Количество всех предсказаний равно числу примеров, умноженному на количество классов\n",
    "\taccuracy = correct_predictions / (n_examples * targets.shape[1])\n",
    "\treturn accuracy, avg_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4PniYIte0fr"
   },
   "source": [
    "Также реализуем функцию для валидации."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CXeRorVGIKre"
   },
   "source": [
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device).float()  # Преобразование targets в float\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Применяем порог 0.5 для бинаризации предсказаний\n",
    "            preds = (outputs > 0.5).int()\n",
    "            \n",
    "            # Сравниваем предсказания с целями по каждому классу\n",
    "            correct_predictions += torch.sum(preds == targets).item()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    # Количество всех предсказаний равно числу примеров, умноженному на количество классов\n",
    "    accuracy = correct_predictions / (n_examples * targets.shape[1])  \n",
    "\n",
    "    return accuracy, avg_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_rdSDBHhhCh"
   },
   "source": [
    "Используя эти две функции, реализуем процедуру дообучения модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xHbOmV_hJ5Ny"
   },
   "source": [
    "path = 'saved_weights.pt'\n",
    "model = model.to(device)\n",
    "\n",
    "if os.path.exists(\n",
    "\t\tpath) and __name__ != '__main__':  # Во время разработки нужно игнорировать файл с весами, запуск готового приложения будет не из этого файла\n",
    "\tprint(\"Файл с весами найден. Загружаем веса вместо обучения.\")\n",
    "\tmodel.load_state_dict(torch.load(path))\n",
    "\thistory = None\n",
    "else:\n",
    "\n",
    "\t# будем записывать значение ошибки и accuracy при обучении и валидации\n",
    "\tfrom collections import defaultdict\n",
    "\n",
    "\thistory = defaultdict(list)\n",
    "\n",
    "\tfor epoch in range(EPOCHS):  # итерация по эпохам\n",
    "\t\tprint(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "\t\tprint('-' * 10)\n",
    "\n",
    "\t\ttrain_acc, train_loss = train_epoch(  # обучение\n",
    "\t\t\tmodel,\n",
    "\t\t\ttrain_data_loader,\n",
    "\t\t\tloss_fn,\n",
    "\t\t\toptimizer,\n",
    "\t\t\tdevice,\n",
    "\t\t\tscheduler,\n",
    "\t\t\tlen(df_train)\n",
    "\t\t)\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\tprint(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\t\tval_acc, val_loss = eval_model(  # валидация\n",
    "\t\t\tmodel,\n",
    "\t\t\tval_data_loader,\n",
    "\t\t\tloss_fn,\n",
    "\t\t\tdevice,\n",
    "\t\t\tlen(df_val)\n",
    "\t\t)\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\t\tprint(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "\n",
    "\t\thistory['train_acc'].append(train_acc)\n",
    "\t\thistory['train_loss'].append(train_loss)\n",
    "\t\thistory['val_acc'].append(val_acc)\n",
    "\t\thistory['val_loss'].append(val_loss)\n",
    "\ttorch.save(model.state_dict(), path)  # сохранение весов модели\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Можем сравнить точность на обучающей и валидационной выборке."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "60vAuburGsBk"
   },
   "source": [
    "if history is not None:\n",
    "\timport matplotlib.pyplot as plt\n",
    "\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\n",
    "\t# График точности (accuracy)\n",
    "\tplt.subplot(1, 2, 1)\n",
    "\tplt.plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "\tplt.plot(history['val_acc'], label='Validation Accuracy', marker='o')\n",
    "\tplt.title('Accuracy Over Epochs')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Accuracy')\n",
    "\tplt.legend()\n",
    "\tplt.ylim([0, 1])\n",
    "\tplt.grid(True)\n",
    "\n",
    "\t# График потерь (loss)\n",
    "\tplt.subplot(1, 2, 2)\n",
    "\tplt.plot(history['train_loss'], label='Train Loss', marker='o', color='r')\n",
    "\tplt.plot(history['val_loss'], label='Validation Loss', marker='o', color='b')\n",
    "\tplt.title('Loss Over Epochs')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kU9MfvTQC9wh"
   },
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnaSaPXMCuBJ"
   },
   "source": [
    "Подсчитаем точность (accuracy) модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jS3gJ_qBEljD"
   },
   "source": [
    "test_acc, _ = eval_model(  # тестирование\n",
    "\tmodel,\n",
    "\ttest_data_loader,\n",
    "\tloss_fn,\n",
    "\tdevice,\n",
    "\tlen(df_test)\n",
    ")\n",
    "\n",
    "test_acc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WL5pDmvFyaU"
   },
   "source": [
    "### Предсказание на произвольных текстах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1cg2r4SvLrT"
   },
   "source": [
    "Теперь нам осталось научиться использовать нашу модель для предсказания класса любого текста.\n",
    "\n",
    " Придумайте свой отзыв и проверьте работу модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QEPi7zQRsDhH"
   },
   "source": "review_text = 'Cпасибо Ярославу за обучение'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBSHv23kvXeO"
   },
   "source": [
    "Для использования нашей модели нам следует токенизировать текст соответствующим образом."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zA5Or4D2sLc9"
   },
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "\treview_text,\n",
    "\tmax_length=MAX_LEN,\n",
    "\tadd_special_tokens=True,\n",
    "\treturn_token_type_ids=False,\n",
    "\tpadding='max_length',\n",
    "\treturn_attention_mask=True,\n",
    "\treturn_tensors='pt',\n",
    "\ttruncation=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foCrgUbJvF1R"
   },
   "source": [
    "Теперь получим предсказания нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Ue9pEDKHmFo"
   },
   "source": [
    "input_ids = encoded_review['input_ids']\n",
    "attention_mask = encoded_review['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = model(input_ids.to(device), attention_mask.to(device))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Применяем пороговое значение для классификации\n",
    "predictions = (output > 0.5)\n",
    "predictions = predictions.to(torch.int)\n",
    "\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Prediction  : {predictions}')\n",
    "print('Предсказанные темы: {}'.format({topic_keywords[i] for i, val in enumerate(predictions.tolist()[0]) if val}))\n",
    "print(output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Примерные значения, которые вы должны уже иметь из своего кода\n",
    "MAX_LEN = 250\n",
    "\n",
    "\n",
    "# Функция для получения предсказания для одного отзыва\n",
    "def predict_review(review_text):\n",
    "\tencoded_review = tokenizer.encode_plus(\n",
    "\t\treview_text,\n",
    "\t\tmax_length=MAX_LEN,\n",
    "\t\tadd_special_tokens=True,\n",
    "\t\treturn_token_type_ids=False,\n",
    "\t\tpadding='max_length',\n",
    "\t\treturn_attention_mask=True,\n",
    "\t\treturn_tensors='pt',\n",
    "\t\ttruncation=True\n",
    "\t)\n",
    "\n",
    "\tinput_ids = encoded_review['input_ids'].to(device)\n",
    "\tattention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_ids, attention_mask)\n",
    "\n",
    "\ttorch.cuda.empty_cache()\n",
    "\n",
    "\tpredictions = (output > 0.5).to(torch.int)\n",
    "\n",
    "\treturn predictions.tolist()[0]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_t = pd.read_csv('test_reviews.csv', index_col=0)\n",
    "\n",
    "# Проход по всему DataFrame и предсказание меток для каждого отзыва\n",
    "for index, row in df_t.iterrows():\n",
    "\treview_text = row['Reviews']\n",
    "\tpredictions = predict_review(review_text)\n",
    "\n",
    "\t# Записываем предсказания в соответствующие столбцы\n",
    "\tdf_t.at[index, 'практика'] = predictions[0]\n",
    "\tdf_t.at[index, 'теория'] = predictions[1]\n",
    "\tdf_t.at[index, 'преподаватель'] = predictions[2]\n",
    "\tdf_t.at[index, 'технологии'] = predictions[3]\n",
    "\tdf_t.at[index, 'актуальность'] = predictions[4]\n",
    "\n",
    "# Теперь DataFrame df_t содержит обновленные метки для каждого отзыва\n",
    "print(df_t)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_t.to_csv('answer_bert.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
